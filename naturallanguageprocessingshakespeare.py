# -*- coding: utf-8 -*-
"""NaturalLanguageProcessingShakespeare.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16F2eBHqr7OcEpTPRP6iIn99bOyKNeioP
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

# Load text data
path_to_file = '/content/shakespeare.txt'
text = open(path_to_file, 'r').read()

# Build character vocabulary
vocab = sorted(set(text))
char_to_ind = {char: ind for ind, char in enumerate(vocab)}
ind_to_char = np.array(vocab)

# Encode text as integer IDs
encoded_text = np.array([char_to_ind[c] for c in text])

# Create TF dataset of input/target character sequences
seq_len = 120
char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)
sequences = char_dataset.batch(seq_len + 1, drop_remainder=True)
def create_seq_targets(seq):
    input_txt = seq[:-1]
    target_txt = seq[1:]
    return input_txt, target_txt
dataset = sequences.map(create_seq_targets)


batch_size = 128
buffer_size = 10000
dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)

# Model parameters
vocab_size = len(vocab)
embed_dim = 64
rnn_neurons = 1026

# Define model
from tensorflow.keras.losses import sparse_categorical_crossentropy
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense
def sparce_cat_loss(y_true, y_pred):
    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)
def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):
    model = Sequential()
    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))
    model.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))
    model.add(Dense(vocab_size))
    model.compile(optimizer='adam', loss=sparce_cat_loss)
    return model

# Create and test model batch
model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size)
for input_example_batch, target_example_batch in dataset.take(1):
    example_batch_predictions = model(input_example_batch)

# Prepare for text generation (load trained weights)
from tensorflow.keras.models import load_model
model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)
model.load_weights('/content/shakespeare_gen.h5')
model.build(tf.TensorShape([1, None]))

# Generate text
def generate_text(model, start_seed, gen_size=500, temp=1.0):
    input_eval = [char_to_ind[s] for s in start_seed]
    input_eval = tf.expand_dims(input_eval, 0)
    text_generated = []
    model.reset_states()
    for i in range(gen_size):
        predictions = model(input_eval)
        predictions = tf.squeeze(predictions, 0) / temp
        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()
        input_eval = tf.expand_dims([predicted_id], 0)
        text_generated.append(ind_to_char[predicted_id])
    return start_seed + "".join(text_generated)

print(generate_text(model, "JENNA", gen_size=10000))
